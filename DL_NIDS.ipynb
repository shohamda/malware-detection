{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL-NIDS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9GyFJqyVTxU"
      },
      "source": [
        "#import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jBY1XPGIDFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c726696-7ded-42db-f783-a9adf22be29e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRriK_meCJcz"
      },
      "source": [
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import operator\n",
        "import datetime\n",
        "\n",
        "from dateutil import parser\n",
        "from collections import defaultdict\n",
        "from sklearn.utils import shuffle\n",
        "from keras.layers import Dense, Dropout, RNN, LSTM, Activation\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from keras.models import Sequential, load_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils.np_utils import to_categorical, normalize\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from timeit import default_timer as timer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQuDEjQiVYnW"
      },
      "source": [
        "#Pre-processin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUS1MXa68Tc9"
      },
      "source": [
        "#Dropping Infinity, NaN, or missing values for each csv the function get\n",
        "def cleanData(inFile, outFile):\n",
        "    count = 1\n",
        "    stats = {}\n",
        "    dropStats = defaultdict(int)\n",
        "    print('cleaning {}'.format(inFile))\n",
        "    with open(inFile, 'r') as csvfile:\n",
        "        data = csvfile.readlines()\n",
        "        totalRows = len(data)\n",
        "        print('total rows read = {}'.format(totalRows))\n",
        "        header = data[0]\n",
        "        for line in data[1:]:\n",
        "            line = line.strip()\n",
        "            cols = line.split(',')\n",
        "            key = cols[-1]\n",
        "            if line.startswith('D') or line.find('Infinity') >= 0 or line.find('infinity') >= 0:\n",
        "                dropStats[key] += 1\n",
        "                continue\n",
        "\n",
        "            dt = parser.parse(cols[2])\n",
        "            # converting timestamps to Unix epoch numeric values.\n",
        "            epochs = (dt - datetime.datetime(1970, 1, 1)).total_seconds()\n",
        "            cols[2] = str(epochs)\n",
        "            line = ','.join(cols)\n",
        "            count += 1\n",
        "\n",
        "            if key in stats:\n",
        "                stats[key].append(line)\n",
        "            else:\n",
        "                stats[key] = [line]\n",
        "    with open(outFile+\".csv\", 'w') as csvoutfile:\n",
        "        csvoutfile.write(header)\n",
        "        with open(outFile + \".stats\", 'w') as fout:\n",
        "            fout.write('Total Clean Rows = {}; Dropped Rows = {}\\n'.format(\n",
        "                count, totalRows - count))\n",
        "            for key in stats:\n",
        "                fout.write('{} = {}\\n'.format(key, len(stats[key])))\n",
        "                line = '\\n'.join(stats[key])\n",
        "                csvoutfile.write('{}\\n'.format(line))\n",
        "                with open('{}-{}.csv'.format(outFile, key), 'w') as labelOut:\n",
        "                    labelOut.write(header)\n",
        "                    labelOut.write(line)\n",
        "            for key in dropStats:\n",
        "                fout.write('Dropped {} = {}\\n'.format(key, dropStats[key]))\n",
        "\n",
        "    print('all done writing {} rows; dropped {} rows'.format(\n",
        "        count, totalRows - count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1G-A4SbVvTE"
      },
      "source": [
        "#Dropping Infinity, NaN, or missing values for all the csvs in the file directory\n",
        "def cleanAllData():\n",
        "    inputDataPath = 'drive/MyDrive/TrafficForML_CICFlowMeter'\n",
        "    outputDataPath = '/content/new_TrafficForML_CICFlowMeter'\n",
        "\n",
        "    if (not os.path.exists(outputDataPath)):\n",
        "        os.mkdir(outputDataPath)\n",
        "\n",
        "    files = os.listdir(inputDataPath)\n",
        "    for file in files:\n",
        "        if file.startswith('.'):\n",
        "            continue\n",
        "        if os.path.isdir(file):\n",
        "            continue\n",
        "        outFile = os.path.join(outputDataPath, file)\n",
        "        inputFile = os.path.join(inputDataPath, file)\n",
        "        cleanData(inputFile, outFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv1Fn66vCU2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf19b2b-53a5-4857-8e29-6992f95b45f4"
      },
      "source": [
        "cleanAllData()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cleaning drive/MyDrive/TrafficForML_CICFlowMeter/03-02-2018.csv\n",
            "total rows read = 1048576\n",
            "all done writing 1044526 rows; dropped 4050 rows\n",
            "cleaning drive/MyDrive/TrafficForML_CICFlowMeter/02-16-2018.csv\n",
            "total rows read = 1048576\n",
            "all done writing 1048575 rows; dropped 1 rows\n",
            "cleaning drive/MyDrive/TrafficForML_CICFlowMeter/02-23-2018.csv\n",
            "total rows read = 1048576\n",
            "all done writing 1042868 rows; dropped 5708 rows\n",
            "cleaning drive/MyDrive/TrafficForML_CICFlowMeter/03-01-2018.csv\n",
            "total rows read = 331126\n",
            "all done writing 328182 rows; dropped 2944 rows\n",
            "cleaning drive/MyDrive/TrafficForML_CICFlowMeter/02-15-2018.csv\n",
            "total rows read = 1048576\n",
            "all done writing 1040549 rows; dropped 8027 rows\n",
            "cleaning drive/MyDrive/TrafficForML_CICFlowMeter/02-22-2018.csv\n",
            "total rows read = 1048576\n",
            "all done writing 1042966 rows; dropped 5610 rows\n",
            "cleaning drive/MyDrive/TrafficForML_CICFlowMeter/02-28-2018.csv\n",
            "total rows read = 613105\n",
            "all done writing 606903 rows; dropped 6202 rows\n",
            "cleaning drive/MyDrive/TrafficForML_CICFlowMeter/14-02-2018.csv\n",
            "total rows read = 1048576\n",
            "all done writing 1044752 rows; dropped 3824 rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--Cn8vxnWl5W"
      },
      "source": [
        "creat a single data file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X48zXVsgWjqV"
      },
      "source": [
        "# script to append all the csv data files we used into 1 (we are using 3 files)\n",
        "dataPath = '/content/drive/MyDrive/new_TrafficForML_CICFlowMeter'\n",
        "\n",
        "fileNames = ['02-15-2018.csv.csv', '02-22-2018.csv.csv', '02-23-2018.csv.csv'] # the files we use\n",
        "\n",
        "df = pd.read_csv(os.path.join(dataPath, fileNames[0]))\n",
        "for name in fileNames[1:]:\n",
        "    fname = os.path.join(dataPath, name)\n",
        "    df1 = pd.read_csv(fname)\n",
        "    df = df.append(df1, ignore_index=True)\n",
        "\n",
        "df = shuffle(df)\n",
        "print('creating multi-class file')\n",
        "outFile = os.path.join(dataPath, 'IDS-2018-multiclass')\n",
        "df.to_csv(outFile + '.csv', index=False)\n",
        "df.to_pickle(outFile + '.pickle')\n",
        "print('creating binary-class file')\n",
        "df['Label'] = df['Label'].map( #mapping the malicious code for labeling\n",
        "    {'Benign': 0, 'FTP-BruteForce': 1, 'SSH-Bruteforce': 1, 'DoS attacks-GoldenEye': 1, 'DoS attacks-Slowloris': 1,\n",
        "     'DoS attacks-SlowHTTPTest': 1, 'DoS attacks-Hulk': 1, 'Brute Force -Web': 1, 'Brute Force -XSS': 1,\n",
        "     'SQL Injection': 1, 'Infilteration': 1, 'Bot': 1})\n",
        "print(df['Label'][1:20])\n",
        "outFile = os.path.join(dataPath, 'IDS-2018-binaryclass')\n",
        "df.to_csv(outFile + '.csv', index=False)\n",
        "df.to_pickle(outFile + '.pickle')\n",
        "print('all done...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c98lqIWWNVD"
      },
      "source": [
        "#CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfJHUosJhu5N"
      },
      "source": [
        "#Creating Path for the results\n",
        "dataPath = '/content/drive/MyDrive/new_TrafficForML_CICFlowMeter'\n",
        "resultPath = '/content/drive/MyDrive/results_keras_tensorflow'\n",
        "\n",
        "if not os.path.exists(resultPath):\n",
        "    print('result path {} created.'.format(resultPath))\n",
        "    os.mkdir(resultPath)\n",
        "    \n",
        "model_name = \"init\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqNSiedKWxvE"
      },
      "source": [
        "#;oading the data into a pickle format\n",
        "def loadData(fileName):\n",
        "    dataFile = os.path.join(dataPath, fileName)\n",
        "    pickleDump = '{}.pickle'.format(dataFile)\n",
        "    if os.path.exists(pickleDump):\n",
        "        df = pd.read_pickle(pickleDump)\n",
        "    else:\n",
        "        df = pd.read_csv(dataFile)\n",
        "        df = df.dropna()\n",
        "        df = shuffle(df)\n",
        "        df.to_pickle(pickleDump)\n",
        "    return df\n",
        "\n",
        "#buliding the CNN model \n",
        "def baseline_model(inputDim=-1, out_shape=(-1,)):\n",
        "    global model_name\n",
        "    model = Sequential()\n",
        "\n",
        "    if inputDim > 0 and out_shape[1] > 0:\n",
        "        model.add(Dense(79, activation='relu', input_shape=(inputDim,))) # layer 1 - 79 nodes\n",
        "        print(f\"out_shape[1]:{out_shape[1]}\")\n",
        "        model.add(Dense(128, activation='relu')) # layer 2 - 128 nodes\n",
        "        model.add(Dense(out_shape[1], activation='softmax')) #This is the output layer - doing softmax to smooth the funtion \n",
        "        \n",
        "        if out_shape[1] > 2:\n",
        "            print('Categorical Cross-Entropy Loss Function')\n",
        "            model_name += \"_categorical\"\n",
        "            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])# using the artical parameters - Adam\n",
        "        else:\n",
        "            model_name += \"_binary\"\n",
        "            print('Binary Cross-Entropy Loss Function')\n",
        "            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvAieMK8W447"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# runing the experiment for the CNN model:\n",
        "def experiment(dataFile, optimizer='adam', epochs=3, batch_size=64):\n",
        "    \n",
        "    #Creating data for analysis\n",
        "    time_gen = int(time.time())\n",
        "    global model_name\n",
        "    model_name = f\"{dataFile}_{time_gen}\"\n",
        "    #$ tensorboard --logdir=logs/\n",
        "    tensorboard = TensorBoard(log_dir='logs/{}'.format(model_name))\n",
        "    \n",
        "    seed = 7\n",
        "    np.random.seed(seed)\n",
        "    cvscores = []\n",
        "    print('optimizer: {} epochs: {} batch_size: {}'.format(optimizer, epochs, batch_size))\n",
        "    \n",
        "    data = loadData(dataFile)\n",
        "    data_y = data.pop('Label')\n",
        "    \n",
        "    #transform named labels into numerical values\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(data_y)\n",
        "    data_y = encoder.transform(data_y)\n",
        "    dummy_y = to_categorical(data_y)\n",
        "    data_x = normalize(data.values)\n",
        "    \n",
        "    #define 5-fold cross validation test harness\n",
        "    inputDim = len(data_x[0])\n",
        "    print('inputdim = ', inputDim)\n",
        "    \n",
        "    #Separate out data\n",
        "    print('data_x', data_x.shape)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data_x, dummy_y, test_size=0.2)\n",
        "    print('X_train, X_test, y_train, y_test', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=7)\n",
        "    start = timer()\n",
        "    #Spliting into train and test sets\n",
        "    for train_index, test_index in sss.split(X=np.zeros(data_x.shape[0]), y=dummy_y):\n",
        "        X_train, X_test = data_x[train_index], data_x[test_index]\n",
        "        y_train, y_test = dummy_y[train_index], dummy_y[test_index]\n",
        "\n",
        "        #create model\n",
        "        print('inputDim',inputDim)\n",
        "        print('y_train.shape', y_train.shape)\n",
        "        model = baseline_model(inputDim, y_train.shape)\n",
        "        print('model',model.summary())\n",
        "    \n",
        "        #train\n",
        "        #print(\"Training \" + dataFile + \" on split \" + str(num))\n",
        "        model.fit(x=X_train, y=y_train, epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard], validation_data=(X_test, y_test))\n",
        "\n",
        "        #save model\n",
        "        model.save(f\"{resultPath}/models/{model_name}.model\")\n",
        "\n",
        "        #num+=1\n",
        "\n",
        "    elapsed = timer() - start\n",
        "    return model, X_test, y_test, elapsed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu1FWnomaf33"
      },
      "source": [
        "def print_statistics(model, X_test, y_test, elapsed):\n",
        "    \n",
        "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    accuracy = accuracy * 100\n",
        "\n",
        "    print(\"Accuracy: {}\".format(accuracy))\n",
        "    print('Elapsed time: {} sec'.format(elapsed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_ivfZBh1s2",
        "outputId": "13d0083e-2789-464f-d18e-78ba27d45a57"
      },
      "source": [
        "print_statistics(*experiment('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-15-2018.csv.csv'))\n",
        "print_statistics(*experiment('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-22-2018.csv.csv'))\n",
        "print_statistics(*experiment('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-23-2018.csv.csv'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "optimizer: adam epochs: 3 batch_size: 64\n",
            "inputdim =  79\n",
            "data_x (1040548, 79)\n",
            "X_train, X_test, y_train, y_test (832438, 79) (208110, 79) (832438, 3) (208110, 3)\n",
            "inputDim 79\n",
            "y_train.shape (832438, 3)\n",
            "out_shape[1]:3\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 79)                6320      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               10240     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 16,947\n",
            "Trainable params: 16,947\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "model None\n",
            "Epoch 1/3\n",
            "13007/13007 - 31s - loss: 0.1335 - accuracy: 0.9640 - val_loss: 0.0891 - val_accuracy: 0.9703\n",
            "Epoch 2/3\n",
            "13007/13007 - 27s - loss: 0.0779 - accuracy: 0.9786 - val_loss: 0.0648 - val_accuracy: 0.9810\n",
            "Epoch 3/3\n",
            "13007/13007 - 27s - loss: 0.0636 - accuracy: 0.9827 - val_loss: 0.0594 - val_accuracy: 0.9818\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/results_keras_tensorflow/models//content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-15-2018.csv.csv_1615413220_categorical.model/assets\n",
            "Accuracy: 98.17644357681274\n",
            "Elapsed time: 98.382833877 sec\n",
            "optimizer: adam epochs: 3 batch_size: 64\n",
            "inputdim =  79\n",
            "data_x (1042965, 79)\n",
            "X_train, X_test, y_train, y_test (834372, 79) (208593, 79) (834372, 4) (208593, 4)\n",
            "inputDim 79\n",
            "y_train.shape (834372, 4)\n",
            "out_shape[1]:4\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 79)                6320      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               10240     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 17,076\n",
            "Trainable params: 17,076\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "model None\n",
            "Epoch 1/3\n",
            "13038/13038 - 28s - loss: 0.0056 - accuracy: 0.9996 - val_loss: 0.0030 - val_accuracy: 0.9997\n",
            "Epoch 2/3\n",
            "13038/13038 - 28s - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9997\n",
            "Epoch 3/3\n",
            "13038/13038 - 27s - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9997\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/results_keras_tensorflow/models//content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-22-2018.csv.csv_1615413332_categorical.model/assets\n",
            "Accuracy: 99.9712347984314\n",
            "Elapsed time: 89.77620513400001 sec\n",
            "optimizer: adam epochs: 3 batch_size: 64\n",
            "inputdim =  79\n",
            "data_x (1042867, 79)\n",
            "X_train, X_test, y_train, y_test (834293, 79) (208574, 79) (834293, 4) (208574, 4)\n",
            "inputDim 79\n",
            "y_train.shape (834293, 4)\n",
            "out_shape[1]:4\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 79)                6320      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               10240     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 17,076\n",
            "Trainable params: 17,076\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "model None\n",
            "Epoch 1/3\n",
            "13036/13036 - 28s - loss: 0.0072 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9995\n",
            "Epoch 2/3\n",
            "13036/13036 - 27s - loss: 0.0045 - accuracy: 0.9995 - val_loss: 0.0044 - val_accuracy: 0.9995\n",
            "Epoch 3/3\n",
            "13036/13036 - 27s - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.0047 - val_accuracy: 0.9995\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/results_keras_tensorflow/models//content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-23-2018.csv.csv_1615413438_categorical.model/assets\n",
            "Accuracy: 99.94582533836365\n",
            "Elapsed time: 89.46267032100008 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpi90s4nXBmS"
      },
      "source": [
        "#RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9RH4_5-gyxH"
      },
      "source": [
        "# Build the RNN model with adam optimizer\n",
        "def build_model(input_dim, output_size=3, optimizer='adam'):\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Embedding(input_dim=input_dim, output_dim=64))\n",
        "\n",
        "  # RNN with 64 layers\n",
        "  model.add(layers.SimpleRNN(64))\n",
        "\n",
        "  #Softmax activation\n",
        "  model.add(Dense(int(output_size), activation='softmax')) # This is the output layer\n",
        "  if (output_size > 2):\n",
        "    model.compile(\n",
        "      loss='categorical_crossentropy',\n",
        "      optimizer=optimizer,\n",
        "      metrics=[\"accuracy\"],)\n",
        "  else:\n",
        "    model.compile(\n",
        "      loss='binary_crossentropy',\n",
        "      optimizer=optimizer,\n",
        "      metrics=[\"accuracy\"],)\n",
        "    \n",
        "  # model.add(layers.Dense(10))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TVUcCVwhCEG"
      },
      "source": [
        "\n",
        "\n",
        "dataPath = '/content/drive/MyDrive/new_TrafficForML_CICFlowMeter'\n",
        "resultPath = '/content/drive/MyDrive/results_keras_tensorflow'\n",
        "model_name = \"rnn\"\n",
        "\n",
        "# runing the experiment for the RNN model (similar to CNN):\n",
        "def experiment_rnn(dataFile, optimizer='adam', epochs=3, batch_size=7000):\n",
        "    seed = 0\n",
        "    np.random.seed(seed)\n",
        "    cvscores = []\n",
        "    print('optimizer: {} epochs: {} batch_size: {}'.format(optimizer, epochs, batch_size))\n",
        "    #loading the data\n",
        "    data = loadData(dataFile)\n",
        "    data_y = data.pop('Label')\n",
        "    #transform named labels into numerical values\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(data_y)\n",
        "    data_y = encoder.transform(data_y)\n",
        "    dummy_y = to_categorical(data_y)\n",
        "    data_x = normalize(data.values)\n",
        "\n",
        "    #define 5-fold cross validation test harness\n",
        "    input_dim = len(data_x[0])\n",
        "\n",
        "    #Split into train and test sets randomly \n",
        "    x_train, x_test, y_train, y_test = train_test_split(data_x, dummy_y, test_size=0.2)\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "    #create the model\n",
        "    model = build_model(input_dim, output_size=y_train.shape[1], optimizer=optimizer)\n",
        "\n",
        "    start = timer()\n",
        "    for train_index, test_index in sss.split(X=np.zeros(data_x.shape[0]), y=dummy_y):\n",
        "        x_train, x_test = data_x[train_index], data_x[test_index]\n",
        "        y_train, y_test = dummy_y[train_index], dummy_y[test_index]\n",
        "\n",
        "        print('y_train.shape', y_train.shape)\n",
        "        print('dataFile', dataFile)\n",
        "        print('model',model.summary())\n",
        "\n",
        "        #train\n",
        "        model.fit(\n",
        "            x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=epochs\n",
        "        )\n",
        "        #save model\n",
        "        model.save(f\"{resultPath}/models/{model_name}.model\")\n",
        "\n",
        "    elapsed = timer() - start\n",
        "    return model, x_test, y_test, elapsed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6I2XbSMh5iy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7799211e-d62e-4b07-8f6c-4a50b682fc41"
      },
      "source": [
        "print_statistics(*experiment_rnn('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-15-2018.csv.csv'))\n",
        "print_statistics(*experiment_rnn('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-22-2018.csv.csv'))\n",
        "print_statistics(*experiment_rnn('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-23-2018.csv.csv'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "optimizer: adam epochs: 3 batch_size: 7000\n",
            "y_train.shape (832438, 3)\n",
            "dataFile /content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-15-2018.csv.csv\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 64)          5056      \n",
            "_________________________________________________________________\n",
            "simple_rnn_3 (SimpleRNN)     (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 13,507\n",
            "Trainable params: 13,507\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "model None\n",
            "Epoch 1/3\n",
            "119/119 [==============================] - 16s 129ms/step - loss: 0.3631 - accuracy: 0.9078 - val_loss: 0.2054 - val_accuracy: 0.9495\n",
            "Epoch 2/3\n",
            "119/119 [==============================] - 15s 129ms/step - loss: 0.2436 - accuracy: 0.9370 - val_loss: 0.2258 - val_accuracy: 0.9495\n",
            "Epoch 3/3\n",
            "119/119 [==============================] - 15s 125ms/step - loss: 0.2258 - accuracy: 0.9496 - val_loss: 0.2257 - val_accuracy: 0.9495\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/results_keras_tensorflow/models/rnn.model/assets\n",
            "Accuracy: 94.95459198951721\n",
            "Elapsed time: 54.772934096000085 sec\n",
            "optimizer: adam epochs: 3 batch_size: 7000\n",
            "y_train.shape (834372, 4)\n",
            "dataFile /content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-22-2018.csv.csv\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 64)          5056      \n",
            "_________________________________________________________________\n",
            "simple_rnn_4 (SimpleRNN)     (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 13,572\n",
            "Trainable params: 13,572\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "model None\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 16s 126ms/step - loss: 0.3236 - accuracy: 0.9009 - val_loss: 0.0052 - val_accuracy: 0.9997\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 15s 125ms/step - loss: 0.0047 - accuracy: 0.9997 - val_loss: 0.0039 - val_accuracy: 0.9997\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 15s 124ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 0.0036 - val_accuracy: 0.9997\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/results_keras_tensorflow/models/rnn.model/assets\n",
            "Accuracy: 99.96500611305237\n",
            "Elapsed time: 54.30627278899999 sec\n",
            "optimizer: adam epochs: 3 batch_size: 7000\n",
            "y_train.shape (834293, 4)\n",
            "dataFile /content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-23-2018.csv.csv\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, None, 64)          5056      \n",
            "_________________________________________________________________\n",
            "simple_rnn_5 (SimpleRNN)     (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 13,572\n",
            "Trainable params: 13,572\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "model None\n",
            "Epoch 1/3\n",
            "120/120 [==============================] - 16s 126ms/step - loss: 0.2321 - accuracy: 0.9995 - val_loss: 0.0060 - val_accuracy: 0.9995\n",
            "Epoch 2/3\n",
            "120/120 [==============================] - 15s 123ms/step - loss: 0.0057 - accuracy: 0.9995 - val_loss: 0.0053 - val_accuracy: 0.9995\n",
            "Epoch 3/3\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.0051 - accuracy: 0.9995 - val_loss: 0.0052 - val_accuracy: 0.9995\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/results_keras_tensorflow/models/rnn.model/assets\n",
            "Accuracy: 99.94582533836365\n",
            "Elapsed time: 54.72602832000007 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}