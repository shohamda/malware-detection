from google.colab import drive
drive.mount('/content/drive')

import csv
import sys
import numpy as np
import tensorflow as tf
import time
import os
import pandas as pd
import operator
import datetime

from dateutil import parser
from collections import defaultdict
from sklearn.utils import shuffle
from keras.layers import Dense, Dropout, RNN, LSTM, Activation
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from keras.models import Sequential, load_model
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical, normalize
from sklearn.utils import shuffle
from tensorflow.keras.callbacks import TensorBoard
from timeit import default_timer as timer

"""#Pre-processin"""

#Dropping Infinity, NaN, or missing values for each csv the function get
def cleanData(inFile, outFile):
    count = 1
    stats = {}
    dropStats = defaultdict(int)
    print('cleaning {}'.format(inFile))
    with open(inFile, 'r') as csvfile:
        data = csvfile.readlines()
        totalRows = len(data)
        print('total rows read = {}'.format(totalRows))
        header = data[0]
        for line in data[1:]:
            line = line.strip()
            cols = line.split(',')
            key = cols[-1]
            if line.startswith('D') or line.find('Infinity') >= 0 or line.find('infinity') >= 0:
                dropStats[key] += 1
                continue

            dt = parser.parse(cols[2])
            # converting timestamps to Unix epoch numeric values.
            epochs = (dt - datetime.datetime(1970, 1, 1)).total_seconds()
            cols[2] = str(epochs)
            line = ','.join(cols)
            count += 1

            if key in stats:
                stats[key].append(line)
            else:
                stats[key] = [line]
    with open(outFile+".csv", 'w') as csvoutfile:
        csvoutfile.write(header)
        with open(outFile + ".stats", 'w') as fout:
            fout.write('Total Clean Rows = {}; Dropped Rows = {}\n'.format(
                count, totalRows - count))
            for key in stats:
                fout.write('{} = {}\n'.format(key, len(stats[key])))
                line = '\n'.join(stats[key])
                csvoutfile.write('{}\n'.format(line))
                with open('{}-{}.csv'.format(outFile, key), 'w') as labelOut:
                    labelOut.write(header)
                    labelOut.write(line)
            for key in dropStats:
                fout.write('Dropped {} = {}\n'.format(key, dropStats[key]))

    print('all done writing {} rows; dropped {} rows'.format(
        count, totalRows - count))

#Dropping Infinity, NaN, or missing values for all the csvs in the file directory
def cleanAllData():
    inputDataPath = 'drive/MyDrive/TrafficForML_CICFlowMeter'
    outputDataPath = '/content/new_TrafficForML_CICFlowMeter'

    if (not os.path.exists(outputDataPath)):
        os.mkdir(outputDataPath)

    files = os.listdir(inputDataPath)
    for file in files:
        if file.startswith('.'):
            continue
        if os.path.isdir(file):
            continue
        outFile = os.path.join(outputDataPath, file)
        inputFile = os.path.join(inputDataPath, file)
        cleanData(inputFile, outFile)

cleanAllData()

"""creat a single data file:"""

# script to append all the csv data files we used into 1 (we are using 3 files)
dataPath = '/content/drive/MyDrive/new_TrafficForML_CICFlowMeter'

fileNames = ['02-15-2018.csv.csv', '02-22-2018.csv.csv', '02-23-2018.csv.csv'] # the files we use

df = pd.read_csv(os.path.join(dataPath, fileNames[0]))
for name in fileNames[1:]:
    fname = os.path.join(dataPath, name)
    df1 = pd.read_csv(fname)
    df = df.append(df1, ignore_index=True)

df = shuffle(df)
print('creating multi-class file')
outFile = os.path.join(dataPath, 'IDS-2018-multiclass')
df.to_csv(outFile + '.csv', index=False)
df.to_pickle(outFile + '.pickle')
print('creating binary-class file')
df['Label'] = df['Label'].map( #mapping the malicious code for labeling
    {'Benign': 0, 'FTP-BruteForce': 1, 'SSH-Bruteforce': 1, 'DoS attacks-GoldenEye': 1, 'DoS attacks-Slowloris': 1,
     'DoS attacks-SlowHTTPTest': 1, 'DoS attacks-Hulk': 1, 'Brute Force -Web': 1, 'Brute Force -XSS': 1,
     'SQL Injection': 1, 'Infilteration': 1, 'Bot': 1})
print(df['Label'][1:20])
outFile = os.path.join(dataPath, 'IDS-2018-binaryclass')
df.to_csv(outFile + '.csv', index=False)
df.to_pickle(outFile + '.pickle')
print('all done...')

"""#CNN model"""

#Creating Path for the results
dataPath = '/content/drive/MyDrive/new_TrafficForML_CICFlowMeter'
resultPath = '/content/drive/MyDrive/results_keras_tensorflow'

if not os.path.exists(resultPath):
    print('result path {} created.'.format(resultPath))
    os.mkdir(resultPath)
    
model_name = "init"

#;oading the data into a pickle format
def loadData(fileName):
    dataFile = os.path.join(dataPath, fileName)
    pickleDump = '{}.pickle'.format(dataFile)
    if os.path.exists(pickleDump):
        df = pd.read_pickle(pickleDump)
    else:
        df = pd.read_csv(dataFile)
        df = df.dropna()
        df = shuffle(df)
        df.to_pickle(pickleDump)
    return df

#buliding the CNN model 
def baseline_model(inputDim=-1, out_shape=(-1,)):
    global model_name
    model = Sequential()

    if inputDim > 0 and out_shape[1] > 0:
        model.add(Dense(79, activation='relu', input_shape=(inputDim,))) # layer 1 - 79 nodes
        print(f"out_shape[1]:{out_shape[1]}")
        model.add(Dense(128, activation='relu')) # layer 2 - 128 nodes
        model.add(Dense(out_shape[1], activation='softmax')) #This is the output layer - doing softmax to smooth the funtion 
        
        if out_shape[1] > 2:
            print('Categorical Cross-Entropy Loss Function')
            model_name += "_categorical"
            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])# using the artical parameters - Adam
        else:
            model_name += "_binary"
            print('Binary Cross-Entropy Loss Function')
            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
# runing the experiment for the CNN model:
def experiment(dataFile, optimizer='adam', epochs=3, batch_size=64):
    
    #Creating data for analysis
    time_gen = int(time.time())
    global model_name
    model_name = f"{dataFile}_{time_gen}"
    #$ tensorboard --logdir=logs/
    tensorboard = TensorBoard(log_dir='logs/{}'.format(model_name))
    
    seed = 7
    np.random.seed(seed)
    cvscores = []
    print('optimizer: {} epochs: {} batch_size: {}'.format(optimizer, epochs, batch_size))
    
    data = loadData(dataFile)
    data_y = data.pop('Label')
    
    #transform named labels into numerical values
    encoder = LabelEncoder()
    encoder.fit(data_y)
    data_y = encoder.transform(data_y)
    dummy_y = to_categorical(data_y)
    data_x = normalize(data.values)
    
    #define 5-fold cross validation test harness
    inputDim = len(data_x[0])
    print('inputdim = ', inputDim)
    
    #Separate out data
    print('data_x', data_x.shape)
    X_train, X_test, y_train, y_test = train_test_split(data_x, dummy_y, test_size=0.2)
    print('X_train, X_test, y_train, y_test', X_train.shape, X_test.shape, y_train.shape, y_test.shape)
    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=7)
    start = timer()
    #Spliting into train and test sets
    for train_index, test_index in sss.split(X=np.zeros(data_x.shape[0]), y=dummy_y):
        X_train, X_test = data_x[train_index], data_x[test_index]
        y_train, y_test = dummy_y[train_index], dummy_y[test_index]

        #create model
        print('inputDim',inputDim)
        print('y_train.shape', y_train.shape)
        model = baseline_model(inputDim, y_train.shape)
        print('model',model.summary())
    
        #train
        #print("Training " + dataFile + " on split " + str(num))
        model.fit(x=X_train, y=y_train, epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[tensorboard], validation_data=(X_test, y_test))

        #save model
        model.save(f"{resultPath}/models/{model_name}.model")

        #num+=1

    elapsed = timer() - start
    return model, X_test, y_test, elapsed

def print_statistics(model, X_test, y_test, elapsed):
    
    _, accuracy = model.evaluate(X_test, y_test, verbose=0)

    accuracy = accuracy * 100

    print("Accuracy: {}".format(accuracy))
    print('Elapsed time: {} sec'.format(elapsed))

print_statistics(*experiment('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-15-2018.csv.csv'))
print_statistics(*experiment('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-22-2018.csv.csv'))
print_statistics(*experiment('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-23-2018.csv.csv'))

"""#RNN model"""

# Build the RNN model with adam optimizer
def build_model(input_dim, output_size=3, optimizer='adam'):
  model = keras.Sequential()
  model.add(layers.Embedding(input_dim=input_dim, output_dim=64))

  # RNN with 64 layers
  model.add(layers.SimpleRNN(64))

  #Softmax activation
  model.add(Dense(int(output_size), activation='softmax')) # This is the output layer
  if (output_size > 2):
    model.compile(
      loss='categorical_crossentropy',
      optimizer=optimizer,
      metrics=["accuracy"],)
  else:
    model.compile(
      loss='binary_crossentropy',
      optimizer=optimizer,
      metrics=["accuracy"],)
    
  # model.add(layers.Dense(10))
  return model

dataPath = '/content/drive/MyDrive/new_TrafficForML_CICFlowMeter'
resultPath = '/content/drive/MyDrive/results_keras_tensorflow'
model_name = "rnn"

# runing the experiment for the RNN model (similar to CNN):
def experiment_rnn(dataFile, optimizer='adam', epochs=3, batch_size=7000):
    seed = 0
    np.random.seed(seed)
    cvscores = []
    print('optimizer: {} epochs: {} batch_size: {}'.format(optimizer, epochs, batch_size))
    #loading the data
    data = loadData(dataFile)
    data_y = data.pop('Label')
    #transform named labels into numerical values
    encoder = LabelEncoder()
    encoder.fit(data_y)
    data_y = encoder.transform(data_y)
    dummy_y = to_categorical(data_y)
    data_x = normalize(data.values)

    #define 5-fold cross validation test harness
    input_dim = len(data_x[0])

    #Split into train and test sets randomly 
    x_train, x_test, y_train, y_test = train_test_split(data_x, dummy_y, test_size=0.2)
    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)
    #create the model
    model = build_model(input_dim, output_size=y_train.shape[1], optimizer=optimizer)

    start = timer()
    for train_index, test_index in sss.split(X=np.zeros(data_x.shape[0]), y=dummy_y):
        x_train, x_test = data_x[train_index], data_x[test_index]
        y_train, y_test = dummy_y[train_index], dummy_y[test_index]

        print('y_train.shape', y_train.shape)
        print('dataFile', dataFile)
        print('model',model.summary())

        #train
        model.fit(
            x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=epochs
        )
        #save model
        model.save(f"{resultPath}/models/{model_name}.model")

    elapsed = timer() - start
    return model, x_test, y_test, elapsed

print_statistics(*experiment_rnn('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-15-2018.csv.csv'))
print_statistics(*experiment_rnn('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-22-2018.csv.csv'))
print_statistics(*experiment_rnn('/content/drive/MyDrive/new_TrafficForML_CICFlowMeter/02-23-2018.csv.csv'))
